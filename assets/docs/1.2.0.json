{"title":"Release 1.2","type":"release","content":"<p>JUL 16, 2025 - Zelin Yu (<a href=\"mailto:yuzelin.yzl@gmail.com\" target=\"_blank\" title=\"undefined\" rel=\"noopener\">yuzelin.yzl@gmail.com</a>)</p>\n<p>The Apache Paimon PMC officially announces the release of Apache Paimon 1.2.0. This version has been developed for \nnearly 3 moths, bringing together the wisdom of more than 50 developers from the global open source community, and \nhas completed more than 260 commits. We sincerely thank all the developers who contributed!</p>\n<h2 id=\"version-overview\">Version Overview</h2><p>Notable changes in this version are:</p>\n<ol>\n<li>Polished Iceberg compatibility, more silky integration with Iceberg.</li>\n<li>Introduce Function to enhance data processing and query capabilities.</li>\n<li>REST Catalog capability further enhanced.</li>\n<li>Postpone bucket (adaptive bucket) Table capability enhancement and bug fix.</li>\n<li>Support for migrating Hudi tables to Paimon tables.</li>\n<li>Continue to enhance the integration with Flink/Spark/Hive, add new features and fix bugs.</li>\n<li>Make multiple optimizations for memory usage to avoid potential OOM issues.</li>\n</ol>\n<h2 id=\"iceberg-compatibility\">Iceberg Compatibility</h2><p>In this version, Iceberg compatibility adds the following capabilities:</p>\n<ol>\n<li><p>Deletion vector compatibility: The file format of Iceberg&#39;s deletion vector is different from Paimon&#39;s, so we \nintroduce a new deletion vector file format. You can set <code>delete-vectors.bitmap64 = true</code> to produce the \nIceberg-compatible delete vector files.</p>\n</li>\n<li><p>Flexible storage location setting: When <code>metadata.iceberg.storage = table-location</code> is set, the Iceberg metadata \nis stored in the table directory, but won&#39;t be registered in Hive/AWS Glue. Therefore, a new option \n<code>metadata.iceberg.storage-location</code> is introduced. When it is set to <code>table-location</code>, the Iceberg metadata is stored \nin the table directory and also registered in Hive/AWS Glue. In this way, you can deployment the data flexibly.</p>\n</li>\n<li><p>Tag support: Now, when a Paimon Tag is created or deleted, the corresponding Iceberg metadata is also changed, and you\ncan access the Tag through Iceberg.</p>\n</li>\n</ol>\n<h2 id=\"function\">Function</h2><p>We introduce the Function interface for better data and query processing. Currently, supporting three types:</p>\n<ol>\n<li>File Function: Provides the Function definition through a File.</li>\n<li>Lambda Function: Defines the Function by Java lambda expression.</li>\n<li>SQL Functionï¼š Defines the Function by Java SQL.</li>\n</ol>\n<p>Examples of Function management by Spark are as follows:</p>\n<p>Create:</p>\n<pre class=\"code\"><code><span class=\"token constant\">CALL</span> sys<span class=\"token punctuation\">.</span><span class=\"token function\">create_function</span><span class=\"token punctuation\">(</span>\n\t`function` <span class=\"token operator\">=</span><span class=\"token operator\">></span> 'my_db<span class=\"token punctuation\">.</span>area_func'<span class=\"token punctuation\">,</span>\n\t`inputParams` <span class=\"token operator\">=</span><span class=\"token operator\">></span> '<span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"name\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"length\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"INT\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"name\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"width\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"INT\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span>'<span class=\"token punctuation\">,</span>\n\t`returnParams` <span class=\"token operator\">=</span><span class=\"token operator\">></span> '<span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"name\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"area\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"BIGINT\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span>'<span class=\"token punctuation\">,</span>\n\t`deterministic` <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">,</span>\n\t`comment` <span class=\"token operator\">=</span><span class=\"token operator\">></span> 'comment'<span class=\"token punctuation\">,</span>\n\t`options` <span class=\"token operator\">=</span><span class=\"token operator\">></span> 'k1<span class=\"token operator\">=</span>v1<span class=\"token punctuation\">,</span>k2<span class=\"token operator\">=</span>v2'\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><p>Modify the definition:</p>\n<pre class=\"code\"><code><span class=\"token constant\">CALL</span> sys<span class=\"token punctuation\">.</span><span class=\"token function\">alter_function</span><span class=\"token punctuation\">(</span>\n  `function` <span class=\"token operator\">=</span><span class=\"token operator\">></span> 'my_db<span class=\"token punctuation\">.</span>area_func'<span class=\"token punctuation\">,</span>\n  `change` <span class=\"token operator\">=</span><span class=\"token operator\">></span> '<span class=\"token punctuation\">{</span><span class=\"token string\">\"action\"</span> <span class=\"token operator\">:</span> <span class=\"token string\">\"addDefinition\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"name\"</span> <span class=\"token operator\">:</span> <span class=\"token string\">\"spark\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"definition\"</span> <span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"type\"</span> <span class=\"token operator\">:</span> <span class=\"token string\">\"lambda\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"definition\"</span> <span class=\"token operator\">:</span> <span class=\"token string\">\"(Integer length, Integer width) -> { return (long) length * width; }\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"language\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"JAVA\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span>'\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><p>Using in query:</p>\n<pre class=\"code\"><code><span class=\"token constant\">SELECT</span> paimon<span class=\"token punctuation\">.</span>my_db<span class=\"token punctuation\">.</span><span class=\"token function\">area_func</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><p>Delete:</p>\n<pre class=\"code\"><code><span class=\"token constant\">CALL</span> sys<span class=\"token punctuation\">.</span><span class=\"token function\">drop_function</span><span class=\"token punctuation\">(</span>`function` <span class=\"token operator\">=</span><span class=\"token operator\">></span> 'my_db<span class=\"token punctuation\">.</span>area_func'<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><p>Currently, the wide-used computing engines don&#39;t support functions well. When they provide better Function interfaces, \nwe can provide a more convenient user interface with the computing engines.</p>\n<h2 id=\"rest-catalog\">REST Catalog</h2><p>This release continues to enhance the REST Catalog, providing the following optimizations and bug fixes:</p>\n<ol>\n<li>Provide row-level and column-level data authentication interfaces.</li>\n<li>Add the following data access interfaces: list tables, list views, list functions.</li>\n<li>Support list object with pattern.</li>\n<li>Provide the snapshot access interface.</li>\n<li>Fix the problem that the table created under REST Catalog cannot read the fallback branch.</li>\n</ol>\n<h2 id=\"postpone-bucket\">Postpone Bucket</h2><p>This version continues to improve the Postpone bucket table capabilities, such as:</p>\n<ol>\n<li>Support deletion vector.</li>\n<li>Support <code>partition.sink-strategy</code> option which improves write performance.</li>\n<li>Paimon CDC supports Postpone bucket table.</li>\n<li>Fix the problem that lookup join with a Postpone bucket table as dimension table produces wrong result.</li>\n<li>Fix possible data error problem of Postpone bucket table write job when the source and sink parallelisms are not the same.</li>\n<li>Fix the problem that the Postpone bucket table cannot be streaming read when <code>changelog-producer = none</code>.</li>\n<li>Fix possible data lost problem if the rescale and compaction jobs of one Postpone bucket table are submitted at the same time. \nThe <code>commit.strict-mode.last-safe-snapshot</code> option is provided to solve it. The job will check the correctness of commit from the \nsnapshot specified by the option. If the job is newly started, you can directly set it to -1.</li>\n</ol>\n<h2 id=\"hudi-migration\">Hudi Migration</h2><p>We provide a Huid table migration tool to support Hudi table easily integrated with the Paimon ecosystem. Currently, only Hudi \ntables registered in HMS are supported. The usage is as follows (through the Flink Jar job):</p>\n<pre class=\"code\"><code><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span>FLINK_HOME<span class=\"token punctuation\">></span></span><span class=\"token operator\">/</span>flink run <span class=\"token punctuation\">.</span>/paimon<span class=\"token operator\">-</span>flink<span class=\"token operator\">-</span>action<span class=\"token operator\">-</span><span class=\"token number\">1.2</span><span class=\"token number\">.0</span><span class=\"token punctuation\">.</span>jar \\\n  clone \\\n  <span class=\"token operator\">--</span>database <span class=\"token keyword\">default</span> \\\n  <span class=\"token operator\">--</span>table hudi_table \\\n  <span class=\"token operator\">--</span>catalog_conf metastore<span class=\"token operator\">=</span>hive \\\n  <span class=\"token operator\">--</span>catalog_conf uri<span class=\"token operator\">=</span>thrift<span class=\"token operator\">:</span><span class=\"token operator\">/</span><span class=\"token operator\">/</span>localhost<span class=\"token operator\">:</span><span class=\"token number\">9088</span> \\\n  <span class=\"token operator\">--</span>target_database test \\\n  <span class=\"token operator\">--</span>target_table test_table \\\n  <span class=\"token operator\">--</span>target_catalog_conf warehouse<span class=\"token operator\">=</span>my_warehouse \\\n  <span class=\"token operator\">--</span>parallelism <span class=\"token number\">10</span> \\\n  <span class=\"token operator\">--</span>where <span class=\"token generics\"><span class=\"token punctuation\">&lt;</span>partition_filter_spec<span class=\"token punctuation\">></span></span></code></pre><h2 id=\"compute-engine-integration-enhancements\">Compute Engine Integration Enhancements</h2><p>This version provides new features and bug fixes of the Flink/Spark/Hive connector:</p>\n<ol>\n<li>Flink lookup join optimization: Previously, all data of the Paimon dimension table needs to be cached in task manager. \n<a href=\"https://cwiki.apache.org/confluence/display/FLINK/FLIP-462+Support+Custom+Data+Distribution+for+Input+Stream+of+Lookup+Join\" target=\"_blank\" title=\"null\" rel=\"noopener\">FLIP-462</a>\nallows to customize the data shuffle mode of the lookup join operator. Paimon implements this optimization, allowing each subtask \nto load part of the dimension table data (instead of full data). In this way, the loading of the dimension table will take less time \nwhen starting the job and the cache will use less memory.</li>\n</ol>\n<p>This optimization requires: i) Using Flink 2.0; ii) The Paimon table is a Fixed bucket table and the join keys contains all bucket keys\n(if not configured, the bucket keys is the same as primary keys). You should use lookup join hints to enable this optimization:</p>\n<pre class=\"code\"><code><span class=\"token operator\">--</span> customers is the <span class=\"token class-name\">Paimon</span> dimension table\n<span class=\"token constant\">SELECT</span> <span class=\"token comment\">/*+ LOOKUP('table'='c', 'shuffle'='true') */</span>\no<span class=\"token punctuation\">.</span>order_id<span class=\"token punctuation\">,</span> o<span class=\"token punctuation\">.</span>total<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">.</span>country<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">.</span>zip\n<span class=\"token constant\">FROM</span> orders <span class=\"token constant\">AS</span> o\n<span class=\"token constant\">JOIN</span> customers\n<span class=\"token constant\">FOR</span> <span class=\"token constant\">SYSTEM_TIME</span> <span class=\"token constant\">AS</span> <span class=\"token constant\">OF</span> o<span class=\"token punctuation\">.</span>proc_time <span class=\"token constant\">AS</span> c\n<span class=\"token constant\">ON</span> o<span class=\"token punctuation\">.</span>customer_id <span class=\"token operator\">=</span> c<span class=\"token punctuation\">.</span>id<span class=\"token punctuation\">;</span></code></pre><ol start=\"2\">\n<li><p>Paimon dimension table supports to be loaded in-memory cache: Previously, Paimon dimension table uses RocksDB as the cache, \nbut its performance is not very good. Therefore, this version introduces purely in-memory cache for dimension table data (note \nthat it may lead to OOM). You can set <code>lookup.cache = memory</code> to enable it.</p>\n</li>\n<li><p>Support V2 write for Spark which reducing serialization overhead and improving write performance. Currently, only fixed bucket \nand append-only (bucket = -1) table are supported. You can set <code>write.use-v2-write = true</code> to enable it.</p>\n</li>\n<li><p>Fix the possible data error problem of Spark bucket join after rescaling bucket.</p>\n</li>\n<li><p>Fix that Hive cannot read/write data of timestamp with local timezone type correctly.</p>\n</li>\n</ol>\n<h2 id=\"memory-usage-optimization\">Memory Usage Optimization</h2><p>Our users have fed back many OOM problems, and we have made some optimizations to solve them:</p>\n<ol>\n<li><p>Optimize the deserialization of the data file statistics to reduce memory usage.</p>\n</li>\n<li><p>For Flink batch jobs, the splits scan are handled in initialization phase in job manager. If the amount of data is large, the job \ninitialization will take a long time and even fail with OOM. To avoid this, you scan set <code>scan.dedicated-split-generation = true</code> to \nlet the splits be scanned in task manager after the job is started.</p>\n</li>\n<li><p>If you write too many partitions at a time to a Postpone bucket table, it is easy to cause OOM. We have optimized the memery usage\nto solve it. </p>\n</li>\n<li><p>When too many partitions data are expired in a single commit, it is possibly to produce OOM. You can set <code>partition.expiration-batch-size</code>\nto specify the limit of maximum partitions can be expired in a single commit to avoid this problem.</p>\n</li>\n</ol>\n<h2 id=\"others\">Others</h2><ol>\n<li>Support default value: The old version of the default value implementation has some defects, and we reimplemented a new version of it.\nSpark and Flink usages are as follows:</li>\n</ol>\n<p>Spark:</p>\n<pre class=\"code\"><code><span class=\"token operator\">--</span> <span class=\"token class-name\">Define</span>\n<span class=\"token constant\">CREATE</span> <span class=\"token class-name\">TABLE</span> my_table <span class=\"token punctuation\">(</span>\n    a <span class=\"token constant\">INT</span><span class=\"token punctuation\">,</span>\n    b <span class=\"token constant\">INT</span> <span class=\"token class-name\">DEFAULT</span> <span class=\"token number\">2</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token operator\">--</span> <span class=\"token class-name\">Modify</span>\n<span class=\"token constant\">ALTER</span> <span class=\"token constant\">TABLE</span> my_table <span class=\"token constant\">ALTER</span> <span class=\"token constant\">COLUMN</span> b <span class=\"token constant\">SET</span> <span class=\"token class-name\">DEFAULT</span> <span class=\"token number\">3</span><span class=\"token punctuation\">;</span></code></pre><p>Flink: Flink SQL does not support default values now, so you should create the table first, then set the default values through Procedure.</p>\n<pre class=\"code\"><code><span class=\"token constant\">CREATE</span> <span class=\"token class-name\">TABLE</span> my_table <span class=\"token punctuation\">(</span>\n    a <span class=\"token constant\">INT</span><span class=\"token punctuation\">,</span>\n    b <span class=\"token constant\">INT</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token constant\">CALL</span> sys<span class=\"token punctuation\">.</span><span class=\"token function\">alter_column_default_value</span><span class=\"token punctuation\">(</span>'<span class=\"token keyword\">default</span><span class=\"token punctuation\">.</span>my_table<span class=\"token char\">', '</span>b<span class=\"token char\">', '</span><span class=\"token number\">2</span>'<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><ol start=\"2\">\n<li><p>Introduce a new time travel option: <code>scan.creation-time-millis</code> to specify a timestamp. If a snapshot is available near the time, starting \nfrom the snapshot. Otherwise, reading files created later than the specified time. This option combines the <code>scan.snapshot-id/scan.timestamp-millis</code> \nand <code>scan.file-creation-time-millis</code>.</p>\n</li>\n<li><p>Support custom partition expiration strategy: You can provide a custom <code>PartitionExpireStrategyFactory</code> and set the table option <code>partition.expiration-strategy = custom</code>\nto activate your partition expiration method.</p>\n</li>\n<li><p>Support custom Flink commit listeners: You can provide multiple custom <code>CommitListenerFactory</code> and set the table option <code>commit.custom-listeners = listener1,listener2,...</code> \nto activate your commit actions at commit phase in a Flink write job.</p>\n</li>\n</ol>\n","toc":[{"depth":2,"text":"Version Overview","id":"version-overview"},{"depth":2,"text":"Iceberg Compatibility","id":"iceberg-compatibility"},{"depth":2,"text":"Function","id":"function"},{"depth":2,"text":"REST Catalog","id":"rest-catalog"},{"depth":2,"text":"Postpone Bucket","id":"postpone-bucket"},{"depth":2,"text":"Hudi Migration","id":"hudi-migration"},{"depth":2,"text":"Compute Engine Integration Enhancements","id":"compute-engine-integration-enhancements"},{"depth":2,"text":"Memory Usage Optimization","id":"memory-usage-optimization"},{"depth":2,"text":"Others","id":"others"}],"alias":"release-1.2","version":"1.2.0","weight":94}